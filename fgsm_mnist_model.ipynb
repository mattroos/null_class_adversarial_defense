{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create figures of exemplar adversarial images\n",
    "\n",
    "Using one set of the models (a model of each null-noise type, and a non-null model), creates some FGSM adverarial images and tests each of the models on those images, displaying results. Also creates some figures like those of the main paper, but only for the single set of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "# from sys import platform\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import import_ipynb\n",
    "from data_generators import mnist_generator\n",
    "from utilities import plot_mnist_digits, create_adversarial_pattern\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RNG seeds, for repeatability\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_titles = 20 # font size for figure titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the baseline model to create a set of adversarial images\n",
    "Adversarial set will contain original source images (taken from the MNIST test set) and\n",
    "corresponding noise images that are added source images (after scaling by some factor, epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate generator\n",
    "null_types = None\n",
    "batch_size = 1\n",
    "\n",
    "# Download the MNIST data located here: http://yann.lecun.com/exdb/mnist/\n",
    "# and set dir_mnist to the location of your downloaded data:\n",
    "dir_mnist = './mnist'\n",
    "# dir_mnist = '/home/mroos/Data/pylearn2data/mnist'\n",
    "\n",
    "gen_data = mnist_generator(dir_mnist, batch_size=batch_size, dataset='test',\n",
    "                           random_order=False, null_types=null_types, p_null_class=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# dir_models = './saved_models_mnist_from_scratch/'\n",
    "dir_models = './saved_models_mnist_sets/'\n",
    "model_filename_baseline = dir_models + 'mnist_model_baseline_00.h5'\n",
    "model_baseline = tf.keras.models.load_model(model_filename_baseline)\n",
    "\n",
    "# Try to create adversarial images from source images. Repeat until\n",
    "# enough of them are found.\n",
    "n_adversarial = 1000\n",
    "images_source = np.empty((n_adversarial, 784))\n",
    "labels_source = np.empty((n_adversarial, 1))\n",
    "grads_signed = np.empty((n_adversarial, 784))\n",
    "epsilon_thresholds = np.empty(n_adversarial)\n",
    "\n",
    "n_acquired = 0\n",
    "while n_acquired < n_adversarial:\n",
    "    im, label, _ = next(gen_data)\n",
    "\n",
    "    # If the model prediction is in error, don't use this image to create an adversarial image.\n",
    "    outputs = model_baseline(im, training=False)\n",
    "    predicted_labels = np.argmax(outputs, axis=1)\n",
    "    if int(predicted_labels[0])!=label[0]:\n",
    "        continue\n",
    "    \n",
    "    # Get the signed gradient that drives the source class towards a *higher* loss\n",
    "    grad_signed, _ = create_adversarial_pattern(model_baseline,\n",
    "                                                   tf.convert_to_tensor(im),\n",
    "                                                   tf.convert_to_tensor(np.array(label)))\n",
    "    \n",
    "    # Create a series of images from the source image, in the direction of the negative gradient.\n",
    "    eps_start = 1e-3\n",
    "    eps_stop = 1.0\n",
    "    epsilons = np.logspace(np.log10(eps_start), np.log10(eps_stop), num=9)\n",
    "    epsilons = np.insert(epsilons, 0, 0)\n",
    "    done_looking = False\n",
    "    while not done_looking:\n",
    "        perturbations = np.expand_dims(epsilons, axis=1) * grad_signed\n",
    "        images = im + perturbations\n",
    "        images = np.clip(images, 0, 1)\n",
    "        outputs = model_baseline(images, training=False)\n",
    "        predicted_labels = np.argmax(outputs, axis=1)\n",
    "        \n",
    "        ix_err = np.where(predicted_labels!=int(label[0]))[0]\n",
    "        if ix_err.size==0:\n",
    "            # No error for largest value in epsilon range. Start over on new source image.\n",
    "            done_looking = True\n",
    "        else:\n",
    "            ix_first = ix_err[0]\n",
    "            if ix_first==0:\n",
    "                raise Exception(\"This line of code should never be hit.\")\n",
    "            elif epsilons[ix_first]-epsilons[ix_first-1] < 1e-3:\n",
    "                # Found threshold within desired tolerance\n",
    "                done_looking = True\n",
    "                epsilon_thresholds[n_acquired] = epsilons[ix_first]\n",
    "                grads_signed[n_acquired] = grad_signed\n",
    "                images_source[n_acquired] = im[0]\n",
    "                labels_source[n_acquired] = label[0]\n",
    "                n_acquired += 1\n",
    "                notification = '%d of %d adversarial images acquired.' % (n_acquired, n_adversarial)\n",
    "                sys.stdout.write('\\r' + notification)\n",
    "                sys.stdout.flush()                \n",
    "            else:\n",
    "                # Do it again, with tighter, denser epsilon range\n",
    "                epsilons = np.logspace(np.log10(epsilons[ix_first-1]), np.log10(epsilons[ix_first]), num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(epsilon_thresholds, 50)\n",
    "median_thresh = np.median(epsilon_thresholds)\n",
    "plt.xlabel('Epsilon threshold')\n",
    "plt.ylabel('Counts')\n",
    "ax = plt.gca()\n",
    "_ = ax.axvline(x=median_thresh, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results from adversarial images (at threshold) for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of adversarial images, right at threshold.\n",
    "images_at_thresh = images_source + np.expand_dims(epsilon_thresholds, axis=1) * grads_signed\n",
    "images_at_thresh = np.clip(images_at_thresh, 0, 1)\n",
    "outputs = model_baseline(images_at_thresh, training=False)\n",
    "plot_mnist_digits(images_at_thresh, labels_source, outputs)\n",
    "fig = plt.gcf()\n",
    "# _ = fig.suptitle('Example results at threshold for a baseline model', fontsize=fs_titles, fontweight='bold')\n",
    "plt.savefig('fig_adversarial_samples_baseline.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results from adversarial images (at 1.5x threshold) for the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of adversarial images, right at threshold.\n",
    "images_at_thresh = images_source + np.expand_dims(1.5*epsilon_thresholds, axis=1) * grads_signed\n",
    "images_at_thresh = np.clip(images_at_thresh, 0, 1)\n",
    "outputs = model_baseline(images_at_thresh, training=False)\n",
    "plot_mnist_digits(images_at_thresh, labels_source, outputs)\n",
    "fig = plt.gcf()\n",
    "# _ = fig.suptitle('Example results at 1.5x threshold for a baseline model', fontsize=fs_titles, fontweight='bold')\n",
    "plt.savefig('fig_adversarial_samples_baseline_1.5.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results from adversarial images (at threshold) for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_null_types = ['u', 's', 'm', 'us', 'um', 'sm', 'usm']\n",
    "\n",
    "images_at_thresh = images_source + np.expand_dims(epsilon_thresholds, axis=1) * grads_signed\n",
    "images_at_thresh = np.clip(images_at_thresh, 0, 1)\n",
    "\n",
    "for null_types in model_null_types:\n",
    "    model_filename = os.path.join(dir_models, 'mnist_model_%s_00.h5' % (null_types))\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "\n",
    "    outputs = model(images_at_thresh, training=False)\n",
    "    \n",
    "    # Tally the number of error of each type\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "    n_correct = np.sum(labels_source[:,0]==predictions)\n",
    "    n_unclass = np.sum(predictions==10)\n",
    "    n_misclass = n_adversarial - n_correct - n_unclass\n",
    "    print('Out of %d samples: %0.1f%% correct, %0.1f%% misclassified, %0.1f%% unclassified.' \\\n",
    "          % (n_adversarial, 100*n_correct/n_adversarial, 100*n_misclass/n_adversarial, 100*n_unclass/n_adversarial))\n",
    "    \n",
    "    plot_mnist_digits(images_at_thresh, labels_source, outputs)\n",
    "    fig = plt.gcf()\n",
    "#     fig.suptitle('Example results at threshold for a model trained with null types: %s' % (null_types), fontsize=fs_titles, fontweight='bold')\n",
    "#     figsize = fig.get_size_inches()\n",
    "#     fig.set_size_inches(figsize[0], 1.2*figsize[1])\n",
    "\n",
    "    plt.savefig('fig_adversarial_samples_%s.png' % (null_types), bbox_inches='tight')\n",
    "    \n",
    "#     time.sleep(1)\n",
    "#     plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run images through model with range of epsilon values, and plot number of prediction errors (null errors, and other-class errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make set of images for each epsilon value\n",
    "step = 0.02\n",
    "epsilons = np.arange(0, 1+step, step)\n",
    "n_epsilons = len(epsilons)\n",
    "adversarial_sets = []\n",
    "for eps in epsilons:\n",
    "    images_at_thresh = images_source + eps * grads_signed\n",
    "    images_at_thresh = np.clip(images_at_thresh, 0, 1)\n",
    "    adversarial_sets.append(images_at_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_errors = np.zeros(n_epsilons, dtype=np.int)\n",
    "n_null_errors = np.zeros(n_epsilons, dtype=np.int)\n",
    "for i_eps in range(n_epsilons):\n",
    "    outputs = model_baseline(adversarial_sets[i_eps], training=False)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "    n_errors[i_eps] = np.sum(predictions!=labels_source[:,0].astype(np.int))\n",
    "    n_null_errors[i_eps] = np.sum(predictions==10)\n",
    "n_other_errors = n_errors - n_null_errors\n",
    "\n",
    "stack_baseline = np.stack((n_other_errors, n_null_errors, n_errors), axis=1)\n",
    "plt.plot(epsilons, stack_baseline)\n",
    "plt.legend(('Misclassifications', 'Nulls', 'Misclassifications+Nulls'))\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Number of errors')\n",
    "plt.ylim((0, n_adversarial+1))\n",
    "fig = plt.gcf()\n",
    "fig.suptitle('Errors vs. epsilon, for baseline model', fontsize=fs_titles, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for null_types in model_null_types:\n",
    "    model_filename = os.path.join(dir_models, 'mnist_model_%s_00.h5' % (null_types))\n",
    "    model = tf.keras.models.load_model(model_filename)\n",
    "\n",
    "    n_errors = np.zeros(n_epsilons, dtype=np.int)\n",
    "    n_null_errors = np.zeros(n_epsilons, dtype=np.int)\n",
    "    for i_eps in range(n_epsilons):\n",
    "        outputs = model(adversarial_sets[i_eps], training=False)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        n_errors[i_eps] = np.sum(predictions!=labels_source[:,0].astype(np.int))\n",
    "        n_null_errors[i_eps] = np.sum(predictions==10)\n",
    "    n_other_errors = n_errors - n_null_errors\n",
    "\n",
    "    stack = np.stack((n_other_errors, n_null_errors, n_errors), axis=1)\n",
    "    if null_types=='usm':\n",
    "        stack_compare_usm = stack\n",
    "    if null_types=='us':\n",
    "        stack_compare_us = stack\n",
    "    if null_types=='s':\n",
    "        stack_compare_s = stack\n",
    "    plt.plot(epsilons, stack)\n",
    "    plt.legend(('Misclassifications', 'Nulls', 'Misclassifications+Nulls'))\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('Number of errors')\n",
    "    plt.ylim((0, n_adversarial*1.05))\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle('Errors vs. epsilon, for model trained on null types: %s' % (null_types), fontsize=fs_titles, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results of baseline and shuffled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epsilons, stack_baseline[:,0], 'r-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_s[:,0], 'g-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_s[:,1], 'g--')\n",
    "plt.plot(epsilons, stack_compare_s[:,2], 'g:')\n",
    "plt.legend(('Baseline model: Misclassifications',\n",
    "        'S model: Misclassifications', 'S model: Nulls', 'S model: Misclassifications+Nulls'))\n",
    "plt.ylim((0, n_adversarial*1.05))\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Error count')\n",
    "plt.title('Baseline vs. Shuffled')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epsilons, stack_baseline[:,0], 'r-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_us[:,0], 'g-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_us[:,1], 'g--')\n",
    "plt.plot(epsilons, stack_compare_us[:,2], 'g:')\n",
    "plt.legend(('Baseline model: Misclassifications',\n",
    "        'US model: Misclassifications', 'US model: Nulls', 'US model: Misclassifications+Nulls'))\n",
    "plt.ylim((0, n_adversarial*1.05))\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Error count')\n",
    "plt.title('Baseline vs. Shuffled+Uniform')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epsilons, stack_baseline[:,0], 'r-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_usm[:,0], 'g-', linewidth=5)\n",
    "plt.plot(epsilons, stack_compare_usm[:,1], 'g--')\n",
    "plt.plot(epsilons, stack_compare_usm[:,2], 'g:')\n",
    "plt.legend(('Baseline model: Misclassifications',\n",
    "        'USM model: Misclassifications', 'USM model: Nulls', 'USM model: Misclassifications+Nulls'))\n",
    "plt.ylim((0, n_adversarial*1.05))\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Error count')\n",
    "plt.title('Baseline vs. Shuffled+Uniform+Mixed')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.suptitle('Comparing Baseline and Shuffled models', fontsize=fs_titles, fontweight='bold')\n",
    "figsize = fig.get_size_inches()\n",
    "fig.set_size_inches(figsize[0], 1.2*figsize[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensorflow2",
   "language": "python",
   "name": "env_tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
