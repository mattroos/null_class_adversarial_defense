{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, ReLU, Softmax, Dropout, Conv2D, MaxPool2D, Flatten, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    e_x = np.exp(x - np.amax(x, axis=axis, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(model, gen_train, gen_test=None, n_epochs=10, model_filename=None, verbose=1):\n",
    "    # If filename is None, best model is not saved.\n",
    "    \n",
    "    dir_model = os.path.dirname(model_filename)    \n",
    "    if not os.path.exists(dir_model):\n",
    "        os.mkdir(dir_model)\n",
    "    \n",
    "    # Set other training parameters\n",
    "    batch_size = 32\n",
    "    n_train_steps_per_epoch = 60000/batch_size\n",
    "    n_test_steps_per_epoch = 10000/batch_size\n",
    "\n",
    "    model_checkpoint_val_loss = ModelCheckpoint(filepath=model_filename,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=verbose,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='auto',\n",
    "                                       save_freq='epoch')\n",
    "\n",
    "    callbacks = [model_checkpoint_val_loss]    \n",
    "    \n",
    "    # Train\n",
    "    t = time.time()\n",
    "    history = model.fit(gen_train, epochs=n_epochs,\n",
    "                                  steps_per_epoch=n_train_steps_per_epoch,\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=gen_test,\n",
    "                                  validation_steps=n_test_steps_per_epoch,\n",
    "                                  use_multiprocessing=False)\n",
    "    print('\\nTime to complete training: %f minutes.\\n' % ((time.time() - t)/60))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ramp(model, gen_train, gen_test=None, n_epochs=10, model_filename=None):\n",
    "    # If filename is None, best model is not saved.\n",
    "    \n",
    "    dir_model = os.path.dirname(model_filename)    \n",
    "    if not os.path.exists(dir_model):\n",
    "        os.mkdir(dir_model)\n",
    "    \n",
    "    # Set other training parameters\n",
    "    batch_size = 32\n",
    "    n_train_steps_per_epoch = 1000\n",
    "    n_test_steps_per_epoch = 1000\n",
    "\n",
    "    model_checkpoint_loss = ModelCheckpoint(filepath=model_filename,\n",
    "                                       monitor='loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='auto',\n",
    "                                       save_freq='epoch')\n",
    "\n",
    "    callbacks = [model_checkpoint_loss]    \n",
    "    \n",
    "    # Train\n",
    "    t = time.time()\n",
    "    history = model.fit_generator(gen_train, epochs=n_epochs,\n",
    "                                  steps_per_epoch=n_train_steps_per_epoch,\n",
    "                                  callbacks=callbacks,\n",
    "                                  use_multiprocessing=False)\n",
    "    print('\\nTime to complete training: %f minutes.\\n' % ((time.time() - t)/60))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_digits(images, labels, predictions=None, n_samples_to_plot=30, width_scale=1.5):\n",
    "    # width_scale: scaling factor so figure width fits nicely in notebook output\n",
    "    \n",
    "    batch_size = len(images)\n",
    "    batch_size = min(batch_size, n_samples_to_plot)\n",
    "    \n",
    "    images = np.reshape(images[:batch_size], (batch_size, 28, 28))\n",
    "    labels = labels[:batch_size]\n",
    "    if predictions is not None:\n",
    "        predictions = softmax(predictions[:batch_size], axis=-1)\n",
    "        \n",
    "    n_sp_cols = min(batch_size, 10)                # number of subplot columns\n",
    "    n_sp_rows = int(np.ceil(batch_size/n_sp_cols)) # number of subplot rows\n",
    "\n",
    "    # extra 1.2 scaling of rows is to accommodate plot titles...\n",
    "    plt.figure(figsize=(width_scale*n_sp_cols, width_scale*n_sp_rows*1.2))\n",
    "    \n",
    "    for i_im  in range(batch_size):\n",
    "        ax = plt.subplot(n_sp_rows, n_sp_cols, i_im+1)\n",
    "        im = images[i_im]\n",
    "\n",
    "        if predictions is not None:\n",
    "            i_max = np.argmax(predictions[i_im])\n",
    "            title_str = '%d, %d, %0.2f' % (labels[i_im], i_max, predictions[i_im, i_max])\n",
    "            if labels[i_im]!=i_max:\n",
    "                # Draw red border around images with wrong prediction, or\n",
    "                # Draw blue border around images with null prediction\n",
    "                if i_max==10:\n",
    "                    chan = 2 # blue\n",
    "                else:\n",
    "                    chan = 0 # red\n",
    "            else:\n",
    "                # Draw green border around images with correct prediction\n",
    "                chan = 1 # green\n",
    "            im = np.repeat(np.expand_dims(im, axis=2), 3, axis=2) # convert to color\n",
    "            im[0:2, :, chan] = 1.0\n",
    "            im[-2:, :, chan] = 1.0\n",
    "            im[:, 0:2, chan] = 1.0\n",
    "            im[:, -2:, chan] = 1.0\n",
    "        else:\n",
    "            title_str = '%d' % (labels[i_im])\n",
    "        \n",
    "        plt.imshow(im, cmap='gray', aspect='equal')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)            \n",
    "        plt.title(title_str)\n",
    "        \n",
    "#     if predictions is not None:\n",
    "#         caption='Values in image titles are: <true label>, <predicted label>, <predicted label score (probability)>.'\n",
    "#     else:\n",
    "#         caption='Values in image titles are ground truth labels.'\n",
    "#     plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    keys = history.history.keys()\n",
    "    loss_keys = [k for k in history.history.keys() if k.endswith('loss')]\n",
    "    acc_keys = [k for k in history.history.keys() if k.endswith('accuracy')]\n",
    "    \n",
    "    figsize = (12, 6)\n",
    "    fontsize = 15\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=figsize)\n",
    "    for k in loss_keys:\n",
    "        ax = plt.plot(history.history[k], label=k)\n",
    "    plt.legend(prop={'size': fontsize})\n",
    "    plt.xlabel('Epoch', fontsize=fontsize)\n",
    "    plt.ylabel('Loss', fontsize=fontsize)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot accuracies\n",
    "    if len(acc_keys)>0:\n",
    "        plt.figure(figsize=figsize)\n",
    "        for k in acc_keys:\n",
    "            ax = plt.plot(history.history[k], label=k)\n",
    "        plt.legend(prop={'size': fontsize})\n",
    "        plt.xlabel('Epoch', fontsize=fontsize)\n",
    "        plt.ylabel('Accuracy', fontsize=fontsize)\n",
    "        plt.tick_params(axis='both', which='major', labelsize=fontsize)\n",
    "        plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial_pattern(model, input_image, input_label):\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_image)\n",
    "        prediction = model(input_image)\n",
    "        loss = loss_object(input_label, prediction)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the input image.\n",
    "    gradient = tape.gradient(loss, input_image)\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return np.array(signed_grad), np.array(gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
